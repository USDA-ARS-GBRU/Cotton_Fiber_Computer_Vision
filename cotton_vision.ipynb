{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Cotton Fiber Object Detection\n",
        "\n",
        "Jeremy Park\n",
        "\n",
        "jipark@ncsu.edu\n",
        "\n",
        "This notebook contains all of the code for cotton fiber tapered-to-hemispherical ratio computer vision predictions.\n",
        "\n",
        "From the downloading of the datasets from Roboflow to the predictions on cotton breeds, this Google Colab notebook handles it all.\n",
        "\n",
        "#### Detectron2 blogs used as inspiration.\n",
        "\n",
        "I used these two blogs to help write the Detectron2 code portions:\n",
        "\n",
        "https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "\n",
        "https://towardsdatascience.com/object-detection-in-6-steps-using-detectron2-705b92575578"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEV2ROpIe1W8"
      },
      "source": [
        "# On Google Drive, mount to your local drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKXmB-BIZ9IB",
        "outputId": "92c6e106-687c-4630-803c-c8ddab7a64e3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uHkV_zCGvyk"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "### Install PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_FzH13EjseR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy7iFW-DZOXZ"
      },
      "source": [
        "### Install Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-i4hmGYk1dL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.8)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "assert torch.__version__.startswith(\"1.8\")   # need to manually install torch 1.8 if Colab changes its default version\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3EosDyAZSs8"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import DatasetMapper, MetadataCatalog, DatasetCatalog, build_detection_test_loader, build_detection_train_loader\n",
        "import os\n",
        "import cv2\n",
        "import itertools\n",
        "import copy\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.structures import BoxMode\n",
        "import detectron2.data.transforms as T\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "import skimage\n",
        "from skimage.transform import rescale\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "import logging\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2.evaluation import inference_context\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "from detectron2.data.transforms import ResizeShortestEdge\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import json\n",
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "import torch\n",
        "import gc\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from math import e\n",
        "from math import ceil\n",
        "from scipy.stats import wasserstein_distance, entropy\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from scipy.special import rel_entr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVu5KtQyG2ux"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT4OGdKzFpgc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "MY_DRIVE = '/content/drive/My Drive'\n",
        "HOME = MY_DRIVE + '/Cotton_Project'\n",
        "MODEL_NAME = \"/m1_69_images_human_AI_3_27_22\"\n",
        "OUTPUT_NAME = \"/Final Product\"\n",
        "DATA_PATH = HOME + \"/Gh_data\" + MODEL_NAME\n",
        "OUTPUT_DIR = HOME + OUTPUT_NAME\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "print(DATA_PATH)\n",
        "print(OUTPUT_DIR)\n",
        "\n",
        "%cd $DATA_PATH\n",
        "%pwd\n",
        "%ls\n",
        "\n",
        "VISUALIZE_TRAIN = False\n",
        "VISUALIZE_KEYENCE = False\n",
        "KEYENCE_RATIO = False\n",
        "RESUME_TRAINING = False\n",
        "TRAINING = False\n",
        "EVALUATE_MODEL = True\n",
        "ROBOFLOW = False\n",
        "TEST_THRESHOLD = 0.7\n",
        "TRAINING_VALIDATION_CURVE = True\n",
        "DISPLAY_BARCHARTS = False\n",
        "DISPLAY_PERCENTAGES = True\n",
        "CALCULATE_RATIOS = False\n",
        "today = datetime.today().strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjv75is7HSWz"
      },
      "source": [
        "# Setting up the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ySTntS_e1XD"
      },
      "source": [
        "### Set up train, val, test folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp6SAHu7HXjk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%cd $DATA_PATH\n",
        "%pwd\n",
        "%ls\n",
        "\n",
        "# printing the number of images in each folder\n",
        "%cd $DATA_PATH\n",
        "%cd './train'\n",
        "%ls -l . | egrep -c '^-'\n",
        "\n",
        "%cd $DATA_PATH\n",
        "%cd './valid'\n",
        "%ls -l . | egrep -c '^-'\n",
        "\n",
        "%cd $DATA_PATH\n",
        "%cd './test'\n",
        "%ls -l . | egrep -c '^-'\n",
        "\n",
        "%cd $DATA_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRASrM9He1XE"
      },
      "source": [
        "### Register each dataset in Detectron2 COCO format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIbAM2pv-urF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
        "register_coco_instances(\"train\", {}, DATA_PATH + \"/train/_annotations.coco.json\", DATA_PATH + \"/train\")\n",
        "register_coco_instances(\"valid\", {}, DATA_PATH + \"/valid/_annotations.coco.json\", DATA_PATH + \"/valid\")\n",
        "register_coco_instances(\"test\", {}, DATA_PATH + \"/test/_annotations.coco.json\", DATA_PATH + \"/test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7w6icD_e1XE"
      },
      "source": [
        "### Visualize example images and annotations in training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph9o8LC5YXId",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "dataset_dicts = DatasetCatalog.get(\"train\")\n",
        "train_metadata = MetadataCatalog.get(\"train\")\n",
        "VISUALIZE_TRAIN=False\n",
        "if VISUALIZE_TRAIN:\n",
        "  for d in random.sample(dataset_dicts, 3):\n",
        "      print(d[\"file_name\"])\n",
        "      img = cv2.imread(d[\"file_name\"])\n",
        "      visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "      vis = visualizer.draw_dataset_dict(d)\n",
        "      cv2_imshow(vis.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfnLaL-qHFV6"
      },
      "source": [
        "# Hooks and Custom Detectron2 trainer\n",
        "\n",
        "Hooks are called at each validation step. So we will use hooks to calculate validation AP.\n",
        "\n",
        "The custom trainer is used to turn off default augmentations inherent to Detectron2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FvCLWQE5nM8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# validation loss code from the help of:\n",
        "# https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
        "class LossEvalHook(HookBase):\n",
        "    def __init__(self, eval_period, model, data_loader):\n",
        "        self._model = model\n",
        "        self._period = eval_period\n",
        "        self._data_loader = data_loader\n",
        "    \n",
        "    def _do_loss_eval(self):\n",
        "        # Copying inference_on_dataset from evaluator.py\n",
        "        total = len(self._data_loader)\n",
        "        num_warmup = min(5, total - 1)\n",
        "        start_time = time.perf_counter()\n",
        "        total_compute_time = 0\n",
        "        losses = []\n",
        "        for idx, inputs in enumerate(self._data_loader):            \n",
        "            if idx == num_warmup:\n",
        "                start_time = time.perf_counter()\n",
        "                total_compute_time = 0\n",
        "            start_compute_time = time.perf_counter()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            total_compute_time += time.perf_counter() - start_compute_time\n",
        "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
        "            seconds_per_img = total_compute_time / iters_after_start\n",
        "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
        "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
        "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
        "                log_every_n_seconds(\n",
        "                    logging.INFO,\n",
        "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
        "                        idx + 1, total, seconds_per_img, str(eta)\n",
        "                    ),\n",
        "                    n=5,\n",
        "                )\n",
        "            loss_batch = self._get_loss(inputs)\n",
        "            losses.append(loss_batch)\n",
        "        mean_loss = np.mean(losses)\n",
        "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
        "        comm.synchronize()\n",
        "        return losses\n",
        "            \n",
        "    def _get_loss(self, data):\n",
        "        # How loss is calculated on train_loop \n",
        "        metrics_dict = self._model(data)\n",
        "        metrics_dict = {\n",
        "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
        "            for k, v in metrics_dict.items()\n",
        "        }\n",
        "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
        "        return total_losses_reduced\n",
        "        \n",
        "    def after_step(self):\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        is_final = next_iter == self.trainer.max_iter\n",
        "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
        "            self._do_loss_eval()\n",
        "        self.trainer.storage.put_scalars(timetest=12)\n",
        "\n",
        "# https://github.com/facebookresearch/detectron2/issues/2114\n",
        "class BestCheckpointer(HookBase):\n",
        "    def before_train(self):\n",
        "        self.best_metric = 0.0\n",
        "        self.logger = logging.getLogger(\"detectron2.trainer\")\n",
        "        self.logger.info(\"######## Running best check pointer\")\n",
        "\n",
        "    def after_step(self):\n",
        "        metric_name = 'bbox/AP50'\n",
        "        if metric_name in self.trainer.storage._history:\n",
        "            eval_metric, batches = self.trainer.storage.history(metric_name)._data[-1]\n",
        "            if self.best_metric < eval_metric:\n",
        "                # add 1 to get to the whole number, just for output purposes\n",
        "                # since it starts at index 0 with iteration 0\n",
        "                new_batch_number = int(batches) + 1\n",
        "                eval_metric = str(round(eval_metric, 2))\n",
        "                if batches % 100 == 0:\n",
        "                  self.logger.info(\"############# New best metric: {}\".format(eval_metric))\n",
        "                self.trainer.checkpointer.save(\"model_{}_iter_{}_with_AP_{:.2f}\".format(today, new_batch_number,eval_metric))\n",
        "\n",
        "# creating our own trainer class to add validation hooks and turn off default augmentations in Detectron2.\n",
        "class CocoTrainer(DefaultTrainer):\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "    if output_folder is None:\n",
        "      os.makedirs(\"coco_eval\", exist_ok=True)\n",
        "      output_folder = \"coco_eval\"\n",
        "    return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
        "  # turning off default augmentations  \n",
        "  def build_hooks(self):\n",
        "    hooks = super().build_hooks()\n",
        "    hooks.insert(-1,LossEvalHook(\n",
        "        cfg.TEST.EVAL_PERIOD,\n",
        "        self.model,\n",
        "        build_detection_test_loader(\n",
        "            self.cfg,\n",
        "            self.cfg.DATASETS.TEST[0],\n",
        "            DatasetMapper(self.cfg,True,augmentations=[])\n",
        "        )\n",
        "    ))\n",
        "    hooks.append(BestCheckpointer())\n",
        "    return hooks\n",
        "\n",
        "  @classmethod\n",
        "  def build_train_loader(cls, cfg):\n",
        "      mapper = DatasetMapper(cfg, is_train=True, augmentations=[])\n",
        "      return build_detection_train_loader(cfg, mapper=mapper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vXXvjYNGYvl"
      },
      "source": [
        "#### Garbage collection to free up RAM, in theory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7fttv7Ptl4m",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "# Train!\n",
        "\n",
        "Perform transfer learning from Faster RCNN R50 FPN 3x model weights, downloaded from Detectron2's model zoo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU1T3ugErCRO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# help from https://towardsdatascience.com/how-to-train-detectron2-on-custom-object-detection-data-be9d1c233e4\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"train\",)\n",
        "cfg.DATASETS.TEST = (\"valid\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.0125\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "cfg.TEST.EVAL_PERIOD = 100\n",
        "cfg.OUTPUT_DIR = OUTPUT_DIR\n",
        "cfg.INPUT.MIN_SIZE_TEST = 1440\n",
        "cfg.INPUT.MAX_SIZE_TEST = 2000\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = CocoTrainer(cfg) \n",
        "trainer.resume_or_load(resume=RESUME_TRAINING)\n",
        "if TRAINING:\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AludKJGxvF0"
      },
      "source": [
        "# Training / Validation Curve "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzdjC4mbCriP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
        "def load_json_arr(json_path):\n",
        "    lines = []\n",
        "    with open(json_path, 'r') as f:\n",
        "        for line in f:\n",
        "            lines.append(json.loads(line))\n",
        "    return lines\n",
        "if TRAINING_VALIDATION_CURVE:\n",
        "  experiment_metrics = load_json_arr(OUTPUT_DIR + '/metrics.json')\n",
        "  plt.plot(\n",
        "      [x['iteration'] for x in experiment_metrics if 'total_loss' in x], \n",
        "      [x['total_loss'] for x in experiment_metrics if 'total_loss' in x])\n",
        "  plt.plot(\n",
        "      [x['iteration'] for x in experiment_metrics if 'validation_loss' in x], \n",
        "      [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x])\n",
        "  plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
        "  plt.xlabel('Iteration', fontsize=14)\n",
        "  plt.ylabel('Loss', fontsize=14)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erqlyRAhaDvR"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lhaK7UEE_bl"
      },
      "source": [
        "#### Function to get the model paths from a given output directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKVT1uz4-Ebw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_model_paths(output_directory):\n",
        "  print(\"Current output directory: \" + output_directory)\n",
        "  file_paths = os.listdir(output_directory) \n",
        "  model_paths = [file_name for file_name in file_paths if \"model_\" in file_name] \n",
        "  return model_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZA3TTlsKrWA"
      },
      "source": [
        "#### Get the best model evaluated on a given dataset from a list of model paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GgaM_i0Ej9W",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def instantiate_model(cfg, model_path, dataset_name, output_dir):\n",
        "  # set the model weights\n",
        "  cfg.MODEL.WEIGHTS = os.path.join(output_dir, model_path)\n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = TEST_THRESHOLD\n",
        "  # prepare the dataset \n",
        "  print(\"dataset_name: \" + dataset_name)\n",
        "  print(cfg.MODEL.WEIGHTS)\n",
        "  cfg.DATASETS.TEST = (dataset_name, )\n",
        "  dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "  val_metadata = MetadataCatalog.get(dataset_name)\n",
        "  # create the predictor\n",
        "  predictor = DefaultPredictor(cfg)\n",
        "  # visualize some predictions\n",
        "  if VISUALIZE_KEYENCE:\n",
        "    for d in random.sample(dataset_dicts, 3): \n",
        "        print(d[\"file_name\"])   \n",
        "        im = cv2.imread(d[\"file_name\"])\n",
        "        outputs = predictor(im)\n",
        "        v = Visualizer(im[:, :, ::-1],\n",
        "                      metadata=val_metadata\n",
        "        )\n",
        "        v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "        cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "\n",
        "        visualizer = Visualizer(im[:, :, ::-1], metadata=val_metadata)\n",
        "        vis = visualizer.draw_dataset_dict(d)\n",
        "        cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "\n",
        "  return cfg, predictor, dataset_dicts, val_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--QGNOh17Jue"
      },
      "source": [
        "### Call COCO Evaluation to get Average Precision scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9tECBQCvMv3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_model(cfg, predictor, dataset_name):\n",
        "  evaluator = COCOEvaluator(dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
        "  val_loader = build_detection_test_loader(cfg, dataset_name, mapper = DatasetMapper(cfg, is_train=False, augmentations=[]))\n",
        "  print(inference_on_dataset(predictor.model, val_loader, evaluator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG4XHqNe7OID"
      },
      "source": [
        "### Calculate T and H ratios with standard deviation for Detectron2 COCO formatted datasets\n",
        "\n",
        "Assuming these datasets were for just one breed: DP90."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMQ9qL9YpmsW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_ratios(predictor, dataset_dicts, val_metadata, test_set_name):\n",
        "  # initialize the count variables\n",
        "  prediction_t = 0\n",
        "  prediction_h = 0\n",
        "  ground_t = 0\n",
        "  ground_h = 0\n",
        "  SANITY_CHECK = False\n",
        "  pred_df = []\n",
        "  ground_df = []\n",
        "  # images are weighed proportional to how much they contribute to the total count of fibers\n",
        "  # for example, an image with 98 fibers should be weighed more in the grand total %T proportion\n",
        "  # than an image with just 2 fibers.\n",
        "  pred_weights = []\n",
        "  ground_weights = []\n",
        "  eval_column_names = [\"Genotype\", \"Image Name\", \"Human Total\", \"ML Total\", \"Human # T\", \"ML # T\", \"Human # H\", \"ML # H\", \"Human % T\", \"ML % T\"]\n",
        "  eval_df = pd.DataFrame(columns = eval_column_names)\n",
        "  if KEYENCE_RATIO:\n",
        "    for d in dataset_dicts: \n",
        "        im = cv2.imread(d[\"file_name\"])\n",
        "        outputs = predictor(im)\n",
        "        # https://github.com/facebookresearch/detectron2/issues/147\n",
        "        prediction_list = outputs['instances'].pred_classes.flatten().tolist()\n",
        "        prediction_counter = Counter(prediction_list)\n",
        "        prediction_h += prediction_counter[1]\n",
        "        prediction_t += prediction_counter[2]\n",
        "        ground_truth_classes = [annotation['category_id'] for annotation in d['annotations']]\n",
        "        ground_truth_counter = Counter(ground_truth_classes)\n",
        "        ground_h += ground_truth_counter[1]\n",
        "        ground_t += ground_truth_counter[2]\n",
        "        current_pred_ratio_sum = prediction_counter[1] + prediction_counter[2]\n",
        "        try:\n",
        "          current_tapered_percentage = round(prediction_counter[2] / current_pred_ratio_sum, 2)\n",
        "        except:\n",
        "          # in the case where there are no predictions\n",
        "          current_tapered_percentage = 0\n",
        "        pred_df.append(current_tapered_percentage)\n",
        "        current_ground_ratio_sum = ground_truth_counter[1] + ground_truth_counter[2]\n",
        "        current_ground_t_percentage = round(ground_truth_counter[2] / current_ground_ratio_sum, 2)\n",
        "        ground_df.append(current_ground_t_percentage)\n",
        "        pred_weights.append(prediction_counter[2])\n",
        "        ground_weights.append(ground_truth_counter[2])\n",
        "        image_path = d[\"file_name\"]\n",
        "        image_name = image_path.rsplit('/', 1)[-1]\n",
        "        image_name = image_name[:-4]\n",
        "        print(\"Image Name:\" + image_name)\n",
        "        eval_data = {\"Genotype\": [test_set_name],\n",
        "                     \"Image Name\": [image_name],\n",
        "                     \"Human Total\": [current_ground_ratio_sum],\n",
        "                     \"ML Total\": [current_pred_ratio_sum],\n",
        "                     \"Human # T\":[ground_truth_counter[2]],\n",
        "                     \"ML # T\":[prediction_counter[2]],\n",
        "                     \"Human # H\":[ground_truth_counter[1]],\n",
        "                     \"ML # H\":[prediction_counter[1]],\n",
        "                     \"Human % T\":[current_ground_t_percentage],\n",
        "                     \"ML % T\":[current_tapered_percentage]}\n",
        "\n",
        "        eval_data = pd.DataFrame(eval_data)\n",
        "        eval_frames = [eval_df, eval_data]\n",
        "        eval_df = pd.concat(eval_frames)\n",
        "        display(eval_df)\n",
        "\n",
        "        if SANITY_CHECK:\n",
        "          print(\"prediction_h: \" + str(prediction_counter[1]))\n",
        "          print(\"prediction_t: \" + str(prediction_counter[2]))\n",
        "          print(\"ground_h: \" + str(ground_truth_counter[1]))\n",
        "          print(\"ground_t: \" + str(ground_truth_counter[2]))\n",
        "          annotations = [annotation['bbox'] for annotation in d['annotations']]\n",
        "          hemispherical_annotations = [annotation['bbox'] for annotation in d['annotations'] if annotation['category_id'] == 1]\n",
        "          tapered_annotations = [annotation['bbox'] for annotation in d['annotations'] if annotation['category_id'] == 2]\n",
        "          # convert to xyxy format\n",
        "          for i in range(len(annotations)):\n",
        "            annotations[i] = BoxMode.convert(annotations[i], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
        "          for i in range(len(hemispherical_annotations)):\n",
        "            hemispherical_annotations[i] = BoxMode.convert(hemispherical_annotations[i], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
        "          for i in range(len(tapered_annotations)):\n",
        "            tapered_annotations[i] = BoxMode.convert(tapered_annotations[i], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
        "          print(\"### Ground Truth ###\")\n",
        "          visualizer = Visualizer(im[:, :, ::-1], metadata=val_metadata)\n",
        "          vis = visualizer.draw_dataset_dict(d)\n",
        "          cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "          print(\"### Predictions ###\")\n",
        "          v = Visualizer(im[:, :, ::-1],\n",
        "                        metadata=val_metadata\n",
        "          )\n",
        "          v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "          cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "\n",
        "    prediction_counter = [0,prediction_h, prediction_t]\n",
        "    ground_truth_counter = [0,ground_h, ground_t]\n",
        "    groundTruth = [ground_truth_counter[2], ground_truth_counter[1]]\n",
        "    predictions = [prediction_counter[2], prediction_counter[1]]\n",
        "    gt_ratio_sum = ground_truth_counter[2] + ground_truth_counter[1]\n",
        "    try:\n",
        "      gt_ratio_t = ground_truth_counter[2] / gt_ratio_sum   \n",
        "      gt_ratio_h = ground_truth_counter[1] / gt_ratio_sum\n",
        "    except:\n",
        "      # if DivideByZeroError, set number of fibers to 0\n",
        "      gt_ratio_t = 0\n",
        "      gt_ratio_h = 0\n",
        "    pred_ratio_sum = prediction_counter[2] + prediction_counter[1]\n",
        "    try:\n",
        "      pred_ratio_t = prediction_counter[2] / pred_ratio_sum \n",
        "      pred_ratio_h = prediction_counter[1] / pred_ratio_sum\n",
        "    # if DivideByZeroError, set number of fibers to 0\n",
        "    except:\n",
        "      pred_ratio_t = 0\n",
        "      pred_ratio_h = 0\n",
        "    gt_ratio_t = str(round(gt_ratio_t * 100, 2))\n",
        "    gt_ratio_h = str(round(gt_ratio_h * 100, 2))\n",
        "    pred_ratio_t = str(round(pred_ratio_t * 100, 2))\n",
        "    pred_ratio_h = str(round(pred_ratio_h * 100, 2))\n",
        "    ratios = [(gt_ratio_t, gt_ratio_h), (pred_ratio_t, pred_ratio_h)]\n",
        "    assert prediction_h == prediction_counter[1]\n",
        "    assert prediction_t == prediction_counter[2]\n",
        "    assert len(pred_weights) == len(ground_weights) == len(dataset_dicts)\n",
        "    assert sum(ground_weights) == ground_t\n",
        "    assert sum(pred_weights) == prediction_t\n",
        "    pred_df = pd.DataFrame(pred_df,columns=['Percent Tapered'])\n",
        "    ground_df = pd.DataFrame(ground_df,columns=['Percent Tapered'])\n",
        "    pred_df_list = pred_df['Percent Tapered'].to_list()\n",
        "    ground_df_list = ground_df['Percent Tapered'].to_list()\n",
        "    try:\n",
        "      pred_weights = [round(current_count / prediction_t, 2) for current_count in pred_weights]\n",
        "    except:\n",
        "      pred_weights = [0 for current_count in pred_weights]\n",
        "    try:\n",
        "      ground_weights = [round(current_count / ground_t, 2) for current_count in ground_weights]\n",
        "    except:\n",
        "      ground_weights = [0 for current_count in ground_weights]\n",
        "    gt_percent_t = round(float(gt_ratio_t) / 100, 3)\n",
        "    gt_percent_h = round(float(gt_ratio_h) / 100, 3)\n",
        "    pred_percent_t = round(float(pred_ratio_t) / 100, 3)\n",
        "    pred_percent_h = round(float(pred_ratio_h) / 100, 3)\n",
        "    print(\"Ground Truth T-H percentages: {} - {}\".format(gt_percent_t, gt_percent_h))\n",
        "    print(\"Prediction T-H ratio: {} - {}\".format(pred_percent_t, pred_percent_h))\n",
        "    # display side by side barcharts comparing ground truth to predictions\n",
        "    if DISPLAY_BARCHARTS:\n",
        "      gt_tapered = [groundTruth[0], \"Tapered\", \"Ground Truth\"]\n",
        "      gt_hemispherical = [groundTruth[1], \"Hemispherical\", \"Ground Truth\"]\n",
        "      pred_tapered = [predictions[0], \"Tapered\", \"Prediction\"]\n",
        "      pred_hemispherical = [predictions[1], \"Hemispherical\", \"Prediction\"]\n",
        "      df = pd.DataFrame([gt_tapered, gt_hemispherical, pred_tapered, pred_hemispherical],columns=('Count', 'Class', 'Type'))\n",
        "      seaborn_plot = sns.catplot(x='Type', y='Count', hue='Class', data=df, kind='bar', palette=sns.color_palette(\"Pastel1\"), color=123, saturation=1)\n",
        "      seaborn_plot.fig.set_size_inches(10,8)\n",
        "      seaborn_plot.fig.subplots_adjust(top=0.81,right=0.86) \n",
        "      ax = seaborn_plot.facet_axis(0,0)\n",
        "      count = 0\n",
        "      percentages = [gt_ratio_t, pred_ratio_t, gt_ratio_h, pred_ratio_h]\n",
        "      # citation:\n",
        "      # https://stackoverflow.com/questions/55586912/seaborn-catplot-set-values-over-the-bars\n",
        "      for p in ax.patches:\n",
        "          height = int(p.get_height())\n",
        "          ax.text(p.get_x() + 0.05, \n",
        "                  p.get_height() * 1.02, \n",
        "                '{} ({}%)'.format(height, percentages[count]), \n",
        "                  color='black', \n",
        "                  rotation='horizontal',\n",
        "                  size='large')\n",
        "          count +=1\n",
        "      # citation\n",
        "      # https://stackoverflow.com/questions/29813694/how-to-add-a-title-to-seaborn-facet-plot\n",
        "      seaborn_plot.fig.subplots_adjust(top=0.9) # adjust the Figure in rp\n",
        "      seaborn_plot.fig.suptitle(\"Ground Truth vs. Predictions with {}-Trained Model on {} Image {} Holdout Set\".format(train_set_name, len(dataset_dicts), test_set_name))\n",
        "    # display histograms of the distributions across images.\n",
        "    if DISPLAY_PERCENTAGES:\n",
        "      # https://www.geeksforgeeks.org/plotting-histogram-in-python-using-matplotlib/\n",
        "      fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(16,8))\n",
        "      axes = plt.gca()\n",
        "      axs = [ax1, ax2]\n",
        "      dfs = [ground_df_list, pred_df_list]\n",
        "      axs[0].set_title(\"GT % T on {} Image {} Test Set\".format(len(dataset_dicts), test_set_name))\n",
        "      axs[1].set_title(\"Pred % T on {} Image {} Test Set\".format(len(dataset_dicts), test_set_name))\n",
        "      (ground_hist, bins_gt, patches) = axs[0].hist(dfs[0], bins=20, range=[0, 1], facecolor='#2a4d69', align='mid', weights = ground_weights)\n",
        "      first_nonzero_index = next((i for i, x in enumerate(ground_hist) if x), None) # x!= 0 for strict match\n",
        "      if first_nonzero_index == 0:\n",
        "        first_nonzero_index == first_nonzero_index\n",
        "      else:\n",
        "        first_nonzero_index -= 1\n",
        "      handles, _ = axs[0].get_legend_handles_labels()\n",
        "      handles.append(axs[0].axvline(gt_percent_t, color='#fe9c8f', linestyle='dashed', linewidth=1))\n",
        "      axs[0].legend(loc='upper left',handles = handles, labels = ['Total GT % T'])\n",
        "      (pred_hist, bins_pred, patches) = axs[1].hist(dfs[1], bins=20, range=[0, 1], facecolor='#854442', align='mid', weights = pred_weights)\n",
        "      first_nonzero_index_pred = next((i for i, x in enumerate(pred_hist) if x), None) # x!= 0 for strict match\n",
        "      if first_nonzero_index_pred == 0:\n",
        "        first_nonzero_index_pred == first_nonzero_index_pred\n",
        "      else:\n",
        "        first_nonzero_index_pred -= 1\n",
        "      largest_x = max(bins_gt[first_nonzero_index], bins_pred[first_nonzero_index_pred])\n",
        "      largest_y = max(max(ground_hist), max(pred_hist))\n",
        "      largest_y = float(ceil(largest_y*100) / 100)\n",
        "      ax1.set_xlim([largest_x,1])\n",
        "      ax2.set_xlim([largest_x,1])\n",
        "      ax1.set_ylim([0,largest_y])\n",
        "      ax2.set_ylim([0,largest_y])\n",
        "      ax1.set_xlabel(\"Tapered Percentage\")\n",
        "      ax2.set_xlabel(\"Tapered Percentage\")\n",
        "      ax1.set_ylabel(\"Proportion\")\n",
        "      ax2.set_ylabel(\"Proportion\")\n",
        "      fig.tight_layout(pad=3.0)\n",
        "      handles, _ = axs[1].get_legend_handles_labels()\n",
        "      handles.append(axs[1].axvline(pred_percent_t, color='#35a79c', linestyle='dashed', linewidth=1))\n",
        "      axs[1].legend(loc='upper left',handles = handles, labels = ['Total Pred % T'])\n",
        "      pred_std_dev = round(pred_df.std(numeric_only=True)['Percent Tapered'], 3)\n",
        "      ground_std_dev = round(ground_df.std(numeric_only=True)['Percent Tapered'], 3)\n",
        "      emd = round(wasserstein_distance(ground_hist, pred_hist),5)\n",
        "      print(\"Prediction Std Dev: {}\".format(pred_std_dev))\n",
        "      print(\"Ground Std Dev: {}\".format(ground_std_dev))\n",
        "      print(\"Earth Mover's Distance: {}\".format(emd))\n",
        "      human_ratio = gt_ratio_t + '-' + gt_ratio_h\n",
        "      ML_ratio = pred_ratio_t + '-' + pred_ratio_h\n",
        "      ratio_diff = abs(float(gt_ratio_t) - float(pred_ratio_t))\n",
        "      data = {\"Genotype\": [\"DP90\"], \"Total Images\": len(dataset_dicts), \"Human T:H Ratio\":[human_ratio], \"Human Total Fibers\": [gt_ratio_sum],\n",
        "              \"ML T:H Ratio\": [ML_ratio], \"ML Total Fibers\": [pred_ratio_sum], \"Ratio Diff\": [ratio_diff],\n",
        "              \"Human Std Dev\":[round(ground_std_dev*100, 1)], \"ML Std Dev\":[round(pred_std_dev*100, 1)]}\n",
        "      keyence_df = pd.DataFrame(data)\n",
        "    \n",
        "    return ratios, keyence_df, eval_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utBetjb8Ae0J"
      },
      "source": [
        "### Calculate Ratio Difference between Ground Truth and Prediction %T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iR8QA5gKqs-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_ratio_difference(groundTruth, prediction):\n",
        "  t_diff = abs(float(groundTruth[0]) - float(prediction[0])) \n",
        "  h_diff = abs(float(groundTruth[1]) - float(prediction[1]))\n",
        "  return t_diff + h_diff\n",
        "\n",
        "def get_ratio_difference(predictor, dataset_dicts, val_metadata, train_set_name, test_set_name):\n",
        "  ratios, keyence_df, eval_df = calculate_ratios(predictor, dataset_dicts, val_metadata, test_set_name)\n",
        "  return calculate_ratio_difference(ratios[0], ratios[1]), keyence_df, eval_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNqxYudOkDrD"
      },
      "source": [
        "# Final Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCieBCmAine"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz8bX2KNkFjp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def model_evaluation(cfg, model_path, dataset_name, train_set_name, test_set_name, output_dir):\n",
        "  current_model_ratio_difference = 10000\n",
        "  cfg, predictor, dataset_dicts, val_metadata = instantiate_model(cfg, model_path, dataset_name, output_dir)\n",
        "  current_model_ratio_difference, keyence_row, eval_data = get_ratio_difference(predictor, dataset_dicts, val_metadata, train_set_name, test_set_name)\n",
        "  evaluate_model(cfg, predictor, dataset_name)\n",
        "  return current_model_ratio_difference, keyence_row, eval_data\n",
        "\n",
        "# pick model based on smallest ratio difference across all images.\n",
        "def best_val_evaluation(model_paths, cfg, dataset_name, train_set_name, test_set_name, output_dir):\n",
        "  best_model_difference = 10000\n",
        "  best_model_path_name = ''\n",
        "  print(\"Current Test Set: \" + test_set_name)\n",
        "  print(\"Current Dataset Dictionaries: \" + dataset_name)\n",
        "  for model_path in model_paths:\n",
        "    print(\"Current Model: \" + model_path)\n",
        "    current_model_ratio_difference, keyence_row = model_evaluation(cfg, model_path, dataset_name, train_set_name, test_set_name, output_dir)\n",
        "    if current_model_ratio_difference < best_model_difference:\n",
        "      best_model_difference = current_model_ratio_difference\n",
        "      best_model_path_name = model_path\n",
        "      print(\"Best Model: \" + model_path)\n",
        "      print(\"Best Model Ratio Difference: {}\".format(best_model_difference))\n",
        "  return best_model_path_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R62WFFpV7-kw"
      },
      "source": [
        "### Evaluate Keyence-Trained Model on Keyence Holdout Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjeJslkQ9Hm3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def final_evaluation(cfg, best_model_path_name, train_set_name, test_set_name, output_dir):\n",
        "  MODEL_PATH_NAME = best_model_path_name.split(\".pth\",1)[0]\n",
        "  DATASET_FILENAME = \"/DP90_total_results_{}_{}.csv\".format(today,MODEL_PATH_NAME)\n",
        "  SUMMARY_DP90_FILENAME = HOME  + DATASET_FILENAME\n",
        "  print(\"Summary results filename: \" + SUMMARY_DP90_FILENAME)\n",
        "  EVAL_DATASET_FILENAME = \"/DP90_per_image_results_{}_{}.csv\".format(today, MODEL_PATH_NAME)\n",
        "  INDIVIDUAL_DP90_FILENAME = HOME + EVAL_DATASET_FILENAME\n",
        "  print(\"Individual image results filename: \" + INDIVIDUAL_DP90_FILENAME)\n",
        "  column_names = [\"Genotype\", \"Total Images\", \"Human T:H Ratio\", \"Human Total Fibers\", \"ML T:H Ratio\", \"ML Total Fibers\",\"Ratio Diff\", \"Human Std Dev\", \"ML Std Dev\"]\n",
        "  geno_df = pd.DataFrame(columns = column_names)\n",
        "  eval_column_names = [\"Genotype\", \"Image Name\", \"Human Total\", \"ML Total\", \"Human # T\", \"ML # T\", \"Human # H\", \"ML # H\", \"Human % T\", \"ML % T\"]\n",
        "  eval_df = pd.DataFrame(columns = eval_column_names)\n",
        "  HOLDOUT_SETS = [\"valid\",\"test\"]\n",
        "\n",
        "  for holdout_set in HOLDOUT_SETS:\n",
        "    test_set_name = ''\n",
        "    if holdout_set is \"test\":\n",
        "      test_set_name = \"Test\"\n",
        "    elif holdout_set is \"valid\":\n",
        "      test_set_name = \"Validation\"  \n",
        "   \n",
        "    diff, keyence_row, eval_data = model_evaluation(cfg, best_model_path_name, holdout_set, train_set_name, test_set_name, output_dir)\n",
        "    frames = [geno_df, keyence_row]\n",
        "    geno_df = pd.concat(frames)\n",
        "    display(geno_df)\n",
        "    eval_frames = [eval_df, eval_data]\n",
        "    eval_df = pd.concat(eval_frames)\n",
        "    display(eval_df)\n",
        "\n",
        "  geno_df.to_csv(SUMMARY_DP90_FILENAME)\n",
        "  eval_df.to_csv(INDIVIDUAL_DP90_FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prVguH3RAqlf"
      },
      "source": [
        "### Run Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb1Sm3Of8DLE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "DISPLAY_BARCHARTS = False\n",
        "CALCULATE_RATIOS = True\n",
        "EVALUATE_MODEL = False\n",
        "dataset_name = \"valid\"\n",
        "train_set_name = \"Keyence\"\n",
        "test_set_name = \"Keyence\"\n",
        "KEYENCE_RATIO = True\n",
        "output_dir = OUTPUT_DIR\n",
        "DISPLAY_BARCHARTS = False\n",
        "CALCULATE_RATIOS = True\n",
        "EVALUATE_MODEL = False\n",
        "best_model_path_name = 'model_weights.pth'\n",
        "final_evaluation(cfg, best_model_path_name, train_set_name, test_set_name, output_dir)\n",
        "# in case you want to find the model with the lowest ratio difference.\n",
        "# best_val_evaluation(model_paths, cfg, dataset_name, train_set_name, test_set_name, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsUHtV3zB6YF"
      },
      "source": [
        "# Evaluate DP90 Model on Final Evaluation Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAXRK3B3i_nB"
      },
      "source": [
        "### Read in CSV with genotype T:H ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfl28jAcB8ys",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "DATASET_FILENAME_DIR = \"/Analysis10_Final\"\n",
        "DATASET_FILENAME = \"/Cotton Test Accession T_H Data v2.csv\"\n",
        "DP90_CSV = '/DP90 Image Variation.csv'\n",
        "COKER312_CSV = '/Coker 312 Image 25-48 Variation.csv'\n",
        "HALF_HALF_CSV = '/Half_Half Image Variation.csv'\n",
        "TH_DATASET = HOME + DATASET_FILENAME_DIR + DATASET_FILENAME\n",
        "COLLABORATION_DIR = \"/content/drive/.shortcut-targets-by-id/1ycJ7eVD7s1isWTBKLA7SgH1qsKQrmM--/\"\n",
        "th_df = pd.read_csv(TH_DATASET)\n",
        "dp90_df = pd.read_csv(HOME + DATASET_FILENAME_DIR + DP90_CSV)\n",
        "coker312_df = pd.read_csv(HOME + DATASET_FILENAME_DIR + COKER312_CSV)\n",
        "half_half_df = pd.read_csv(HOME + DATASET_FILENAME_DIR + HALF_HALF_CSV)\n",
        "dp90_df=dp90_df.rename(columns={\"Image Nam/ROI coords\":\"Image Name\", 'Image #T':\"#T\", 'Image #H':\"#H\", '%T':\"%T\",'%H':\"%H\",'Total Fiber':'Total'})\n",
        "coker312_df=coker312_df.rename(columns={\"Image Nam/ROI coords\":\"Image Name\", 'Image #T':\"#T\", 'Image #H':\"#H\", '%T':\"%T\",'%H':\"%H\",'Total Fiber':'Total'})\n",
        "half_half_df=half_half_df.rename(columns={\"Image Name; ROI coords\":\"Image Name\", 'Image #T':\"#T\", 'Image #H':\"#H\", '%T':\"%T\",'%H':\"%H\",'Total Fiber':'Total'})\n",
        "frames = [dp90_df, coker312_df, half_half_df]\n",
        "greenhouse_df = pd.concat(frames)\n",
        "th_df = th_df[['Genotype','Drive Path', '%T', '%H', '#T', '#H']]\n",
        "# things change in the path name.\n",
        "th_df['Drive Path'] = th_df['Drive Path'].str.replace('>', \"/\")\n",
        "th_df['Drive Path'] = th_df['Drive Path'].str.replace('Test Accession Images', \"Evaluation Test Accession Images\")\n",
        "th_df['Drive Path'] =th_df['Drive Path'].str.replace('Gb cv Pima 3-79', \"Gb cv Pima 379\")\n",
        "th_df['Drive Path'] =th_df['Drive Path'].str.replace('Gh cv Half and Half Test Set', \"\")\n",
        "th_df['Drive Path'] =th_df['Drive Path'].str.replace('Gh cv Coker 312 Test Set', \"\")\n",
        "th_df['Drive Path'] =th_df['Drive Path'].str.replace('Gh cv Deltapine 90 Test Set', \"\")\n",
        "th_df['Drive Path'] =th_df['Drive Path'].str.replace('Gb cv Phytogen 800 Test Set', \"\")\n",
        "th_df['Drive Path'] =th_df['Drive Path'].str.replace('Gb cv Pima 379 Test Set', \"\")\n",
        "th_df['Drive Path'] =th_df['Drive Path'].str.replace('Gb cv Pima S7 Test Set', \"\")\n",
        "th_df['Drive Path'] = th_df['Drive Path'].str.replace('Modeling', \"Machine Learning\")\n",
        "display(th_df)\n",
        "display(greenhouse_df)\n",
        "%cd $COLLABORATION_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWO-zIhke1XI"
      },
      "source": [
        "### Create a jpg for an image if no jpg exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc3TmbI3G2iL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# citation:\n",
        "# https://stackoverflow.com/questions/28870504/converting-tiff-to-jpeg-in-python\n",
        "def create_jpg(genotype_image_dir):\n",
        "  if any(File.endswith(\".jpg\") for File in os.listdir(genotype_image_dir)):\n",
        "      return\n",
        "  else:\n",
        "    yourpath = genotype_image_dir\n",
        "    for root, dirs, files in os.walk(yourpath, topdown=False):\n",
        "        for name in files:\n",
        "            print(os.path.join(root, name))\n",
        "            if os.path.splitext(os.path.join(root, name))[1].lower() == \".tif\":\n",
        "                if os.path.isfile(os.path.splitext(os.path.join(root, name))[0] + \".jpg\"):\n",
        "                    print(\"A jpeg file already exists for %s\" % name)\n",
        "                # If a jpeg is *NOT* present, create one from the tiff.\n",
        "                else:\n",
        "                    outfile = os.path.splitext(os.path.join(root, name))[0] + \".jpg\"\n",
        "                    try:\n",
        "                        im = Image.open(os.path.join(root, name))\n",
        "                        print(\"Generating jpeg for %s\" % name)\n",
        "                        im.thumbnail(im.size)\n",
        "                        im.save(outfile, \"JPEG\", quality=100)\n",
        "                    except:\n",
        "                        print(\"FAIL!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ98YBbOe1XJ"
      },
      "source": [
        "### Function to return all image paths with .jpg ending in a given directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcw2-9foD-W0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_image_paths(current_genotype_dir):\n",
        "  current_genotype_dir = current_genotype_dir.strip()\n",
        "  create_jpg(current_genotype_dir)\n",
        "  file_paths = os.listdir(current_genotype_dir)\n",
        "  image_paths = [current_genotype_dir + \"/\" + file_name for file_name in file_paths if \".jpg\" in file_name] \n",
        "  return image_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdYdQ5Tse1XJ"
      },
      "source": [
        "### Calculate the ratios for the evaluation set data.\n",
        "\n",
        "This code is slightly different than the Detectron2 formatted data, since we are now accepting CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZFfps9Mp2XK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_genotype_ratios(predictor, row, val_metadata, train_set_name, greenhouse_df):\n",
        "  # initialize the count variables\n",
        "  prediction_t = 0\n",
        "  prediction_h = 0\n",
        "  ground_t = 0\n",
        "  ground_h = 0\n",
        "  SANITY_CHECK = False\n",
        "  pred_df = []\n",
        "  ground_df = []\n",
        "  # weigh an image proportional to how much it contributes to the overall count.\n",
        "  pred_weights = []\n",
        "  ground_weights = []\n",
        "  current_genotype_dir = COLLABORATION_DIR + row['Drive Path']\n",
        "  print(\"Current Genotype: \" + row['Genotype'])\n",
        "  image_paths = get_image_paths(current_genotype_dir)\n",
        "  GB = 'Gb' in row['Genotype']\n",
        "  eval_column_names = [\"Genotype\", \"Image Name\", \"Human Total\", \"ML Total\", \"Human # T\", \"ML # T\", \"Human # H\", \"ML # H\", \"Human % T\", \"ML % T\"]\n",
        "  eval_df = pd.DataFrame(columns = eval_column_names)\n",
        "  # for all images of that genotype\n",
        "  if KEYENCE_RATIO:\n",
        "    for image_path in image_paths: \n",
        "        im = cv2.imread(image_path)\n",
        "        im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "        im = np.stack((im,)*3, axis=-1)\n",
        "        outputs = predictor(im)\n",
        "        # https://github.com/facebookresearch/detectron2/issues/147\n",
        "        prediction_list = outputs['instances'].pred_classes.flatten().tolist()\n",
        "        prediction_counter = Counter(prediction_list)\n",
        "        prediction_h += prediction_counter[1]\n",
        "        prediction_t += prediction_counter[2]\n",
        "        image_name = image_path.rsplit('/', 1)[-1]\n",
        "        image_name = image_name[:-4]\n",
        "        print(\"Image Name:\" + image_name)\n",
        "        current_h = 0\n",
        "        current_t = 0\n",
        "        ground_total = 30\n",
        "        # if GB, we know that the total T count is 30.\n",
        "        if not GB:\n",
        "          image_row = greenhouse_df[greenhouse_df['Image Name'].str.contains(image_name)]\n",
        "          image_row = image_row.iloc[0]\n",
        "          current_h = int(image_row['#H'])\n",
        "          current_t = int(image_row['#T'])\n",
        "          assert int(image_row['Total']) == int(image_row['#H'])+int(image_row['#T'])\n",
        "          ground_total = int(image_row['Total'])\n",
        "        else:\n",
        "          current_h = 0\n",
        "          current_t = 30\n",
        "          ground_total = 30\n",
        "\n",
        "        ground_h += current_h\n",
        "        ground_t += current_t\n",
        "        current_pred_ratio_sum = prediction_counter[1] + prediction_counter[2]\n",
        "        current_tapered_percentage = round(prediction_counter[2] / current_pred_ratio_sum, 2)\n",
        "        pred_df.append(current_tapered_percentage)\n",
        "        current_ground_ratio_sum = ground_total\n",
        "        current_ground_t_percentage = round(current_t / current_ground_ratio_sum, 2)\n",
        "        if not GB:\n",
        "          assert \"{:.1f}\".format(float(image_row['%T'])/100) == \"{:.1f}\".format(float(current_ground_t_percentage))\n",
        "        ground_df.append(current_ground_t_percentage)\n",
        "        # add the count to the weights list\n",
        "        pred_weights.append(prediction_counter[2])\n",
        "        ground_weights.append(current_t)\n",
        "\n",
        "        if SANITY_CHECK:\n",
        "          v = Visualizer(im[:, :, ::-1],\n",
        "                        metadata=val_metadata, scale = 0.5\n",
        "          )\n",
        "          print(\"### Predictions for {} ###\".format(image_name))\n",
        "          v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "          cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "          print(\"Prediction H Count: \" + str(prediction_counter[1]))\n",
        "          print(\"Prediction T Count: \" + str(prediction_counter[2]))\n",
        "          print(\"Human Label H Count: \" + str(current_h))\n",
        "          print(\"Human Label T Count: \" + str(current_t))\n",
        "\n",
        "        eval_data = {\"Genotype\": [row['Genotype']],\n",
        "                     \"Image Name\": [image_name],\n",
        "                     \"Human Total\": [ground_total],\n",
        "                     \"ML Total\": [current_pred_ratio_sum],\n",
        "                     \"Human # T\":[current_t],\n",
        "                     \"ML # T\":[prediction_counter[2]],\n",
        "                     \"Human # H\":[current_h],\n",
        "                     \"ML # H\":[prediction_counter[1]],\n",
        "                     \"Human % T\":[current_ground_t_percentage],\n",
        "                     \"ML % T\":[current_tapered_percentage]}\n",
        "        eval_data = pd.DataFrame(eval_data)\n",
        "        eval_frames = [eval_df, eval_data]\n",
        "        eval_df = pd.concat(eval_frames)\n",
        "        display(eval_df)\n",
        "\n",
        "    prediction_counter = [0,prediction_h, prediction_t]\n",
        "    ground_truth_counter = [0,ground_h, ground_t]\n",
        "    groundTruth = [ground_truth_counter[2], ground_truth_counter[1]]\n",
        "    predictions = [prediction_counter[2], prediction_counter[1]]\n",
        "    gt_ratio_sum = ground_truth_counter[2] + ground_truth_counter[1]\n",
        "    gt_ratio_t = ground_truth_counter[2] / gt_ratio_sum   \n",
        "    gt_ratio_h = ground_truth_counter[1] / gt_ratio_sum\n",
        "    pred_ratio_sum = prediction_counter[2] + prediction_counter[1]\n",
        "    pred_ratio_t = prediction_counter[2] / pred_ratio_sum \n",
        "    pred_ratio_h = prediction_counter[1] / pred_ratio_sum\n",
        "    print(\"GT Total: \".format(gt_ratio_sum))\n",
        "    print(\"Pred Total: \".format(pred_ratio_sum))\n",
        "    gt_ratio_t = str(round(gt_ratio_t * 100, 2))\n",
        "    gt_ratio_h = str(round(gt_ratio_h * 100, 2))\n",
        "    pred_ratio_t = str(round(pred_ratio_t * 100, 2))\n",
        "    pred_ratio_h = str(round(pred_ratio_h * 100, 2))\n",
        "    ratios = [(gt_ratio_t, gt_ratio_h), (pred_ratio_t, pred_ratio_h)]\n",
        "    assert prediction_h == prediction_counter[1]\n",
        "    assert prediction_t == prediction_counter[2]\n",
        "    assert len(pred_weights) == len(ground_weights) == len(image_paths)\n",
        "    assert sum(ground_weights) == ground_t\n",
        "    assert sum(pred_weights) == prediction_t\n",
        "    assert \"{}\".format(round(float(row['%H']))) == \"{}\".format(round(float(gt_ratio_h)))\n",
        "    assert \"{}\".format(round(float(row['%T']))) == \"{}\".format(round(float(gt_ratio_t)))\n",
        "    pred_df = pd.DataFrame(pred_df,columns=['Percent Tapered'])\n",
        "    ground_df = pd.DataFrame(ground_df,columns=['Percent Tapered'])\n",
        "    pred_df_list = pred_df['Percent Tapered'].to_list()\n",
        "    ground_df_list = ground_df['Percent Tapered'].to_list()\n",
        "    print(\"Image Paths:\")\n",
        "    print(image_paths)\n",
        "    print(\"prediction T count per image:\")\n",
        "    print(pred_weights)\n",
        "    print(\"human label T count per image:\")\n",
        "    print(ground_weights)\n",
        "    pred_weights = [round(current_count / prediction_t, 2) for current_count in pred_weights]\n",
        "    ground_weights = [round(current_count / ground_t, 2) for current_count in ground_weights]\n",
        "    assert sum(ground_weights) > 0.95\n",
        "    assert sum(pred_weights) > 0.95\n",
        "    print(\"prediction weights per image:\")\n",
        "    print(pred_weights)\n",
        "    print(\"human label weights per image:\")\n",
        "    print(ground_weights)\n",
        "    gt_percent_t = round(float(gt_ratio_t) / 100, 2)\n",
        "    gt_percent_h = round(float(gt_ratio_h) / 100, 2)\n",
        "    pred_percent_t = round(float(pred_ratio_t) / 100, 2)\n",
        "    pred_percent_h = round(float(pred_ratio_h) / 100, 2)\n",
        "    print(\"Ground Truth T-H ratio: {} - {}\".format(gt_percent_t, gt_percent_h))\n",
        "    print(\"Prediction T-H ratio: {} - {}\".format(pred_percent_t, pred_percent_h))\n",
        "    # compare human vs ml in barcharts per genotype\n",
        "    if DISPLAY_BARCHARTS:\n",
        "      gt_tapered = [groundTruth[0], \"Tapered\", \"Ground Truth\"]\n",
        "      gt_hemispherical = [groundTruth[1], \"Hemispherical\", \"Ground Truth\"]\n",
        "      pred_tapered = [predictions[0], \"Tapered\", \"Prediction\"]\n",
        "      pred_hemispherical = [predictions[1], \"Hemispherical\", \"Prediction\"]\n",
        "      df = pd.DataFrame([gt_tapered, gt_hemispherical, pred_tapered, pred_hemispherical],columns=('Count', 'Class', 'Type'))\n",
        "      # citation:\n",
        "      # https://stackoverflow.com/questions/55586912/seaborn-catplot-set-values-over-the-bars\n",
        "      seaborn_plot = sns.catplot(x='Type', y='Count', hue='Class', data=df, kind='bar', palette=sns.color_palette(\"Pastel1\"), color=123, saturation=1)\n",
        "      seaborn_plot.fig.set_size_inches(10,8)\n",
        "      seaborn_plot.fig.subplots_adjust(top=0.81,right=0.86) \n",
        "      ax = seaborn_plot.facet_axis(0,0)\n",
        "      count = 0\n",
        "      percentages = [gt_ratio_t, pred_ratio_t, gt_ratio_h, pred_ratio_h]\n",
        "      for p in ax.patches:\n",
        "          height = int(p.get_height())\n",
        "          ax.text(p.get_x() + 0.05, \n",
        "                  p.get_height() * 1.02, \n",
        "                '{} ({}%)'.format(height, percentages[count]),  \n",
        "                  color='black', \n",
        "                  rotation='horizontal',\n",
        "                  size='large')\n",
        "          count +=1\n",
        "      # citation\n",
        "      # https://stackoverflow.com/questions/29813694/how-to-add-a-title-to-seaborn-facet-plot\n",
        "      seaborn_plot.fig.subplots_adjust(top=0.9)\n",
        "      seaborn_plot.fig.suptitle(\"Ground Truth vs. Predictions with {}-Trained Model on {} Image {} Holdout Set\".format(train_set_name, len(dataset_dicts), test_set_name))\n",
        "    # compare T distributions across images for human and ML output\n",
        "    if DISPLAY_PERCENTAGES:\n",
        "      # https://www.geeksforgeeks.org/plotting-histogram-in-python-using-matplotlib/\n",
        "      fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(16,8))\n",
        "      axes = plt.gca()\n",
        "      axs = [ax1, ax2]\n",
        "      dfs = [ground_df_list, pred_df_list]\n",
        "      axs[0].set_title(\"{}: GT % T on {} Images\".format(row['Genotype'],len(image_paths)))\n",
        "      axs[1].set_title(\"{}: Pred % T on {} Images\".format(row['Genotype'],len(image_paths)))\n",
        "      (ground_hist, bins_gt, patches) = axs[0].hist(dfs[0], bins=10, range=[0, 1], facecolor='#2a4d69', align='mid', weights = ground_weights)\n",
        "      first_nonzero_index = next((i for i, x in enumerate(ground_hist) if x), None) # x!= 0 for strict match\n",
        "      if first_nonzero_index == 0:\n",
        "        first_nonzero_index == first_nonzero_index\n",
        "      else:\n",
        "        first_nonzero_index -= 1\n",
        "      handles, _ = axs[0].get_legend_handles_labels()\n",
        "      handles.append(axs[0].axvline(gt_percent_t, color='#fe9c8f', linestyle='dashed', linewidth=1))\n",
        "      axs[0].legend(loc='upper left',handles = handles, labels = ['Total GT % T'])\n",
        "      (pred_hist, bins_pred, patches) = axs[1].hist(dfs[1], bins=10, range=[0, 1], facecolor='#854442', align='mid', weights = pred_weights)\n",
        "      first_nonzero_index_pred = next((i for i, x in enumerate(pred_hist) if x), None) # x!= 0 for strict match\n",
        "      if first_nonzero_index_pred == 0:\n",
        "        first_nonzero_index_pred == first_nonzero_index_pred\n",
        "      else:\n",
        "        first_nonzero_index_pred -= 1\n",
        "      largest_x = max(bins_gt[first_nonzero_index], bins_pred[first_nonzero_index_pred])\n",
        "      largest_y = max(max(ground_hist), max(pred_hist))\n",
        "      largest_y = float(ceil(largest_y*10) / 10)\n",
        "      ax1.set_xlim([largest_x,1])\n",
        "      ax2.set_xlim([largest_x,1])\n",
        "      ax1.set_ylim([0,largest_y])\n",
        "      ax2.set_ylim([0,largest_y])\n",
        "      ax1.set_xlabel(\"Tapered Percentage\")\n",
        "      ax2.set_xlabel(\"Tapered Percentage\")\n",
        "      ax1.set_ylabel(\"Proportion\")\n",
        "      ax2.set_ylabel(\"Proportion\")\n",
        "      fig.tight_layout(pad=3.0)\n",
        "      handles, _ = axs[1].get_legend_handles_labels()\n",
        "      # https://stackoverflow.com/questions/24988448/how-to-draw-vertical-lines-on-a-given-plot\n",
        "      handles.append(axs[1].axvline(pred_percent_t, color='#35a79c', linestyle='dashed', linewidth=1))\n",
        "      axs[1].legend(loc='upper left',handles = handles, labels = ['Total Pred % T'])\n",
        "      pred_std_dev = round(pred_df.std(numeric_only=True)['Percent Tapered'], 2)\n",
        "      ground_std_dev = round(ground_df.std(numeric_only=True)['Percent Tapered'], 2)\n",
        "      # metric of how different two distributions are:\n",
        "      # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html\n",
        "      emd = round(wasserstein_distance(ground_hist, pred_hist),2)\n",
        "      print(\"Prediction Std Dev: {}\".format(pred_std_dev))\n",
        "      print(\"Ground Std Dev: {}\".format(ground_std_dev))\n",
        "      print(\"Earth Mover's Distance: {}\".format(emd))\n",
        "      human_ratio = gt_ratio_t + '-' + gt_ratio_h\n",
        "      ML_ratio = pred_ratio_t + '-' + pred_ratio_h\n",
        "      ratio_diff = abs(float(gt_ratio_t) - float(pred_ratio_t))\n",
        "      column_names = [\"Genotype\", \"Human T:H Ratio\", \"Human Total Fibers\", \"ML T:H Ratio\", \"ML Total Fibers\",\"Ratio Diff\", \"Human Std Dev\", \"ML Std Dev\"]\n",
        "      data = {\"Genotype\": [row['Genotype']], \"Human T:H Ratio\":[human_ratio], \"Human Total Fibers\": [gt_ratio_sum],\n",
        "              \"ML T:H Ratio\": [ML_ratio], \"ML Total Fibers\": [pred_ratio_sum], \"Ratio Diff\": [ratio_diff],\n",
        "              \"Human Std Dev\":[round(ground_std_dev*100, 1)], \"ML Std Dev\":[round(pred_std_dev*100, 1)]}\n",
        "      data_hist = {\"Genotype\": [row['Genotype']], \"Human Total %T Mean\": [gt_percent_t],\"ML Total %T Mean\": [pred_percent_t], \"Human Hist\":[list(ground_hist)], \"ML Hist\": [list(pred_hist)], \"Bins\": [10], \"Range\":[[0, 1]]}\n",
        "      # Convert the dictionary into DataFrame\n",
        "      geno_df = pd.DataFrame(data)\n",
        "      data_hist = pd.DataFrame(data_hist)\n",
        "\n",
        "    return ratios, geno_df, data_hist, eval_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW7oyPzle1XJ"
      },
      "source": [
        "### Calculate ratio differences between humans and ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFLkpQF7mj4g",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_ratio_difference(groundTruth, prediction):\n",
        "  t_diff = abs(float(groundTruth[0]) - float(prediction[0])) \n",
        "  h_diff = abs(float(groundTruth[1]) - float(prediction[1]))\n",
        "  return t_diff + h_diff\n",
        "\n",
        "def genotype_ratio_difference(predictor, row, val_metadata, train_set_name, greenhouse_df):\n",
        "  ratios, geno_df,data_hist, eval_df = get_genotype_ratios(predictor, row, val_metadata, train_set_name, greenhouse_df)\n",
        "  return calculate_ratio_difference(ratios[0], ratios[1]), geno_df,data_hist, eval_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4eQRfGgmj4g"
      },
      "source": [
        "# Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMAbKwb3mj4h",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def genotype_evaluation(cfg, best_model_path_name, holdout_set, th_df, output_dir, train_set_name, greenhouse_df):\n",
        "  current_model_ratio_difference = 10000\n",
        "  # instantiate the model\n",
        "  cfg, predictor, dataset_dicts, val_metadata = instantiate_model(cfg, best_model_path_name, dataset_name, output_dir)\n",
        "  display(th_df)\n",
        "  ratio_list = []\n",
        "  # create dataset file names.\n",
        "  MODEL_PATH_NAME = best_model_path_name.split(\".pth\",1)[0]\n",
        "  DATASET_FILENAME = \"/greenhouse_total_results_{}_{}.csv\".format(today,MODEL_PATH_NAME)\n",
        "  SUMMARY_GREENHOUSE_FILENAME = HOME  + DATASET_FILENAME\n",
        "  HIST_DATASET_FILENAME = \"/hist_greenhouse_{}_{}.csv\".format(today,MODEL_PATH_NAME)\n",
        "  HIST_GREENHOUSE_NAME = HOME + HIST_DATASET_FILENAME\n",
        "  EVAL_DATASET_FILENAME = \"/greenhouse_per_image_results_{}_{}.csv\".format(today,MODEL_PATH_NAME)\n",
        "  INDIVIDUAL_DP90_FILENAME = HOME + EVAL_DATASET_FILENAME\n",
        "  print(SUMMARY_GREENHOUSE_FILENAME)\n",
        "  print(HIST_GREENHOUSE_NAME)\n",
        "  print(INDIVIDUAL_DP90_FILENAME)\n",
        "  column_names = [\"Genotype\", \"Human T:H Ratio\", \"Human Total Fibers\", \"ML T:H Ratio\", \"ML Total Fibers\",\"Ratio Diff\", \"Human Std Dev\", \"ML Std Dev\"]\n",
        "  geno_df = pd.DataFrame(columns = column_names)\n",
        "  hist_column_names = [\"Genotype\",\"Human Total %T Mean\",\"ML Total %T Mean\", \"Human Hist\", \"ML Hist\", \"Bins\",\"Range\"]\n",
        "  hist_df = pd.DataFrame(columns = hist_column_names)\n",
        "  eval_column_names = [\"Genotype\", \"Image Name\", \"Human Total\", \"ML Total\", \"Human # T\", \"ML # T\", \"Human # H\", \"ML # H\", \"Human % T\", \"ML % T\"]\n",
        "  eval_df = pd.DataFrame(columns = eval_column_names)\n",
        "\n",
        "  if CALCULATE_RATIOS:\n",
        "    th_df = th_df.reset_index()  # make sure indexes pair with number of rows\n",
        "    for index, row in th_df.iterrows():\n",
        "      current_model_ratio_difference, genotype_row, data_hist, eval_data = genotype_ratio_difference(predictor, row, val_metadata, train_set_name, greenhouse_df)\n",
        "      ratio_list.append(current_model_ratio_difference)\n",
        "      frames = [geno_df, genotype_row]\n",
        "      geno_df = pd.concat(frames)\n",
        "      display(geno_df)\n",
        "      hist_frames = [hist_df, data_hist]\n",
        "      hist_df = pd.concat(hist_frames)\n",
        "      display(hist_df)\n",
        "      eval_frames = [eval_df, eval_data]\n",
        "      eval_df = pd.concat(eval_frames)\n",
        "      display(eval_df)\n",
        "  geno_df.to_csv(SUMMARY_GREENHOUSE_FILENAME)  \n",
        "  hist_df.to_csv(HIST_GREENHOUSE_NAME)  \n",
        "  eval_df.to_csv(INDIVIDUAL_DP90_FILENAME)  \n",
        "  return ratio_list\n",
        "\n",
        "# grid search for best model and best confidence threshold.\n",
        "def best_genotype_evaluation(cfg, model_paths, holdout_set, th_df, output_dir, train_set_name, thresholds, greenhouse_df):\n",
        "  best_model_difference = 10000\n",
        "  best_model_path_name = ''\n",
        "  best_threshold = 1\n",
        "  for model_path in model_paths:\n",
        "    if model_path is not '':\n",
        "      for threshold in thresholds:\n",
        "        TEST_THRESHOLD = threshold\n",
        "        print(\"Current Model: \" + model_path)\n",
        "        print(\"Current Output Dir: \" + output_dir)\n",
        "        print(\"Current threshold: {}\".format(TEST_THRESHOLD))\n",
        "        ratio_list = genotype_evaluation(cfg, model_path, holdout_set, th_df, output_dir, train_set_name, greenhouse_df)\n",
        "\n",
        "        if sum(ratio_list) < best_model_difference:\n",
        "          best_model_difference = sum(ratio_list)\n",
        "          best_model_path_name = model_path\n",
        "          best_threshold = threshold\n",
        "          print(\"Best Model: \" + best_model_path_name)\n",
        "          print(\"Best Model Ratio Difference: {}\".format(best_model_difference))\n",
        "          print(\"Best Threshold: {}\".format(best_threshold))\n",
        "  return best_model_path_name, best_model_difference, best_threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIFjmthXJVRV"
      },
      "source": [
        "# Final Greenhouse Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtXixG6hmj4h",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def genotype_final(cfg, best_model_path_name, th_df, output_dir, train_set_name, greenhouse_df):\n",
        "   # this doesn't matter, I don't think.\n",
        "  holdout_set = \"test\"\n",
        "  genotype_evaluation(cfg, best_model_path_name, holdout_set, th_df, output_dir, train_set_name, greenhouse_df)\n",
        "\n",
        "output_dirs = [OUTPUT_DIR]\n",
        "for output_dir in output_dirs:\n",
        "  DISPLAY_BARCHARTS = False\n",
        "  CALCULATE_RATIOS = True\n",
        "  EVALUATE_MODEL = False\n",
        "  dataset_name = \"test\"\n",
        "  train_set_name = \"Keyence\"\n",
        "  test_set_name = \"Keyence\"\n",
        "  KEYENCE_RATIO = True\n",
        "  output_dir = OUTPUT_DIR\n",
        "  DISPLAY_BARCHARTS = False\n",
        "  CALCULATE_RATIOS = True\n",
        "  EVALUATE_MODEL = False\n",
        "  genotype_final(cfg, best_model_path_name, th_df, output_dir, train_set_name, greenhouse_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QHnVupBBn9eR"
      ],
      "name": "cotton_vision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}